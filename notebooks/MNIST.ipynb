{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParticleEM/ParEM_neural_latent_variable_model/blob/master/notebooks/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNEy6xRqKKxZ"
      },
      "source": [
        "# Description\n",
        "\n",
        "Introductory blabla\n",
        "\n",
        "### Dataset description:\n",
        "\n",
        "Dataset consists of $M$ images $y = (y^{m})_{m=1}^M$.\n",
        "\n",
        "### Model description:\n",
        "\n",
        "$$\n",
        "p_\\theta (x,y) = \\prod_{m=1}^M p_\\theta(x^{m}, y^{m})\n",
        "$$\n",
        "where\n",
        "$$\n",
        "p_\\theta(x,y)= p_\\theta(y|x)p(x)\n",
        "$$\n",
        "with\n",
        "$$\n",
        "p_\\theta(y|x) = \\mathcal{N}(y|\\mu_\\theta(x), \\sigma^2 I),\n",
        "$$\n",
        "where $\\mu_\\theta(\\cdot)$ is a neural network parameterised by $\\theta$, and $p(x) = \\mathcal{N}(x|0,I)$. \n",
        "\n",
        "The neural net consists of \n",
        "\n",
        "$$\\mu_\\theta =  \\tanh\\circ c_\\theta\\circ d_\\theta \\circ d_\\theta \\circ proj_\\theta$$\n",
        "\n",
        "where\n",
        "\n",
        "\n",
        "*   $\\phi$ is a GELU activation function.\n",
        "*   $c_\\theta$ is a transpose convolutional layer.\n",
        "*   $proj_\\theta=\\phi \\circ c_\\theta \\circ \\phi\\circ l_\\theta$. Maps from $\\mathbb{R}^{D_x}$ to $4\\times 16\\times 16$, where $D_x$ is dimension of latent variable.\n",
        "*   $l_\\theta$ is a linear layer.\n",
        "*   $d_\\theta=\\phi \\circ conv_\\theta \\circ \\phi\\circ conv_\\theta + I$ (HAS A SKIP CONNECTION).\n",
        "*   $conv_\\theta$ is a convolutional layer.\n",
        "\n",
        "\n",
        "### Algorithm description:\n",
        "\n",
        "For $k=1,\\dots,K$.\n",
        "\\begin{align*}\n",
        "    \\theta_{k+1} &= \\theta_k + \\frac{h}{N}\\sum_{n=1}^N \\sum_{m\\in\\mathcal{I}} \\nabla_\\theta \\log p_{\\theta_k}\n",
        "(X^{n,m}_k, y^{m}) \\\\\n",
        "X^{n,m}_{k+1}&=X^{n,m}_k + h\\nabla_x \\log p_{\\theta_k}\n",
        "(X^{n,m}_k, y^{m}) + \\sqrt{2h} W^{n,m}_k \\quad \\forall m = 1, .., M, n= 1,..., N.\n",
        "\\end{align*}\n",
        "\n",
        "where $\\mathcal{I}$ is a random subset of $M_b$ images in $\\mathcal{D}$.\n",
        "\n",
        "\n",
        "Describe stopping criterion: early stop bla bla.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceG5fAENPfr6"
      },
      "source": [
        "# Import modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz47uMXqKYf5"
      },
      "source": [
        "First, we load the modules we need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ssu1R-nnaGpW"
      },
      "outputs": [],
      "source": [
        "# Declare dicitonary like object for storing config variables:\n",
        "import argparse\n",
        "args = argparse.Namespace()\n",
        "args.seed = 1 # Seed for PRNs\n",
        "\n",
        "# Data setttings\n",
        "args.n_images = 10000 # M\n",
        "\n",
        "# Training settings\n",
        "args.n_epochs = 500 # K\n",
        "args.n_batch = 128 # M_b\n",
        "args.n_sampler_batch = 750\n",
        "args.early_stopping = True # Turn on early stopping\n",
        "\n",
        "# Model Settings\n",
        "args.x_dim = 10 # D_x\n",
        "args.theta_opt = 'rmsprop' # Lambda premultiplying matrix\n",
        "args.likelihood_var = 0.3 ** 2 # \\sigma^2\n",
        "\n",
        "# EM Settings\n",
        "args.theta_step_size = 1e-3 # h_\\theta\n",
        "args.q_step_size = 5e-5 # h_q\n",
        "args.clip_grad = False\n",
        "args.n_particles = 10 # N\n",
        "\n",
        "# Synthesis settings\n",
        "args.corrupt_std = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLCqb7XBKj2b"
      },
      "outputs": [],
      "source": [
        "# Install missing modules\n",
        "%%capture\n",
        "!pip install torchtyping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpxVMzKzQGaN"
      },
      "outputs": [],
      "source": [
        "# Import standard modules\n",
        "import torch\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X55L4TEtl385",
        "outputId": "b9bea7bf-b1d8-4d82-c012-f8bc36af2694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ParEM_VAE'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 291 (delta 26), reused 56 (delta 23), pack-reused 230\u001b[K\n",
            "Receiving objects: 100% (291/291), 42.93 KiB | 4.77 MiB/s, done.\n",
            "Resolving deltas: 100% (134/134), done.\n"
          ]
        }
      ],
      "source": [
        "# Import custom modules\n",
        "!rm -rf ParEM_VAE\n",
        "!git clone https://pareem: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@github.com/ParticleEM/ParEM_VAE.git\n",
        "sys.path.append(\"/content/ParEM_VAE/\")\n",
        "from parem.model import G\n",
        "from parem.pga import PGA, optimisers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C83FbhuRRXc_"
      },
      "source": [
        "# Set paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqT0ajsqRVKi",
        "outputId": "9a84074e-6deb-4941-fd6c-00b0db9af9a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mounts drive to VM in colab.\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=False)\n",
        "\n",
        "# Path where checkpoints will be saved:\n",
        "CHECKPOINT_DIR = Path(\"/content/gdrive/MyDrive/particle-em/mnist\")\n",
        "CHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbrwaBFyRb4P"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXOewTu5Piuw",
        "outputId": "f6218e46-5119-4be9-bd05-b92e8ab2947e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ]
        }
      ],
      "source": [
        "#@title Load dataset\n",
        "from parem.mnist import get_mnist\n",
        "\n",
        "dataset = get_mnist('/content/mnist', args.n_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "JtEGiGKkS0pm"
      },
      "outputs": [],
      "source": [
        "#@title Divvy up dataset in batches for training.\n",
        "\n",
        "train = torch.utils.data.DataLoader(dataset, batch_size=args.n_batch, shuffle=True, pin_memory=True)\n",
        "larger_batch_train = torch.utils.data.DataLoader(dataset, batch_size=args.n_sampler_batch, shuffle=True, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue1zy2hKjmlb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtyping import TensorType\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "optimisers = {'sgd': torch.optim.SGD,\n",
        "              'adagrad': torch.optim.Adagrad,\n",
        "              'rmsprop': torch.optim.RMSprop,\n",
        "              }\n",
        "\n",
        "\n",
        "class PGA:\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 n_images: int,\n",
        "                 dl,\n",
        "                 q_step_size: float = 1e-2,\n",
        "                 theta_step_size: float = 1e-2,\n",
        "                 n_particles: int = 30,\n",
        "                 device=\"cpu\",\n",
        "                 clip_grad=False,\n",
        "                 theta_opt='sgd',):\n",
        "\n",
        "        self.n_particles = n_particles\n",
        "        self.model = model\n",
        "        self.q_step_size = q_step_size\n",
        "        self.device = device\n",
        "        self.clip_grad = clip_grad\n",
        "        self.dl = dl\n",
        "        self.n_images = n_images\n",
        "\n",
        "        # Initialize samples\n",
        "        self._particles = model.init_x([n_images, n_particles],\n",
        "                                       device=self.device)\n",
        "\n",
        "        # Declare theta optimiser\n",
        "        if type(theta_opt) == str:\n",
        "            self.theta_opt = optimisers[theta_opt](model.parameters(),\n",
        "                                                   lr=theta_step_size)\n",
        "        elif isinstance(theta_opt, torch.optim.Optimizer):\n",
        "            self.theta_opt = theta_opt\n",
        "\n",
        "    def loss(self,\n",
        "             images,#: TensorType[\"n_batch\", \"image_dimensions\": ...],\n",
        "             particles: TensorType[\"n_batch\", \"n_particles\",\"x_dim\"]\n",
        "             ) -> TensorType[()]:\n",
        "        \"\"\"\n",
        "        \\frac{M}{N|images|}\\sum_{n=1}^N\\sum_{m in images}p_{\\theta_k}(X_k^{n,m}, y^m)\n",
        "        \"\"\"\n",
        "        log_p = self.model.log_p_v(images, particles)\n",
        "        assert not log_p.isnan().any(), \"log_p is nan.\"\n",
        "        return - (1. / images.shape[0]) * log_p.mean()\n",
        "\n",
        "    def step(self,\n",
        "             img_batch,#: TensorType[\"n_batch\", \"image_dimensions\":...],\n",
        "             idx: TensorType[\"n_batch\"]):\n",
        "\n",
        "        # Compute theta gradients:\n",
        "        self.model.train()  # ??\n",
        "        self.theta_opt.zero_grad()  # Zero theta gradients\n",
        "        self.model = self.model.requires_grad_(True)  # ??\n",
        "\n",
        "        # Evaluate loss function:\n",
        "        loss = self.loss(img_batch, self._particles[idx].to(img_batch.device))\n",
        "\n",
        "        # Backpropagate theta gradients:\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip theta gradients if clipping requested:\n",
        "        if self.clip_grad:\n",
        "            clip_grad_norm_(self.model.parameters(), 100)\n",
        "\n",
        "        # Update particles batch by batch (s.t. device memory is not exceeded):\n",
        "        self.model.eval()\n",
        "        self.model = self.model.requires_grad_(False)\n",
        "        for imgs, idx in self.dl:\n",
        "            # Select particles to be updated in this iteration:\n",
        "            sub_particles = (self._particles[idx].detach().clone()\n",
        "                                 .to(img_batch.device).requires_grad_(True))\n",
        "            # Send relevant images to device:\n",
        "            imgs = imgs.to(img_batch.device)\n",
        "\n",
        "            # Compute x gradients:\n",
        "            log_p_v = self.model.log_p_v(imgs, sub_particles).sum()\n",
        "            x_grad = torch.autograd.grad(log_p_v, sub_particles)[0]\n",
        "\n",
        "            # Take a gradient step for this batch's particles:\n",
        "            self._particles[idx] += (self.q_step_size\n",
        "                                     * x_grad.to(self._particles.device))\n",
        "\n",
        "        # Add noise to all particles:\n",
        "        self._particles += ((2 * self.q_step_size) ** 0.5\n",
        "                            * torch.randn_like(self._particles))\n",
        "\n",
        "        # Update theta:\n",
        "        self.theta_opt.step()\n",
        "\n",
        "        # Return value of loss function:\n",
        "        return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rduJTaL4RPB7"
      },
      "outputs": [],
      "source": [
        "model = G(args.x_dim, sigma2=args.likelihood_var, nc=1, use_bn=True).to(DEVICE)\n",
        "pga = PGA(model,\n",
        "          args.n_images,\n",
        "          larger_batch_train,\n",
        "          device='cpu',\n",
        "          theta_step_size=args.theta_step_size,\n",
        "          q_step_size=args.q_step_size,\n",
        "          n_particles=args.n_particles,\n",
        "          clip_grad=args.clip_grad,\n",
        "          theta_opt=args.theta_opt,\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2uiBWF1KPSu"
      },
      "outputs": [],
      "source": [
        "# Import modules necessary for training loop\n",
        "%%capture\n",
        "!pip install wandb\n",
        "import wandb\n",
        "import pickle\n",
        "from torchvision.utils import make_grid\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB6ZLj815WfN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Plotting function\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "def show(imgs):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, dpi=400)\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.detach()\n",
        "        img = F.to_pil_image(img)\n",
        "        axs[0, i].imshow(np.asarray(img))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "weq8_THjRHw-",
        "outputId": "7e779355-a9d5-4877-86f8-2dbb730ba2d2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjenninglim\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220808_004200-1iwybtub</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/jenninglim/particle-em-mnist/runs/1iwybtub\" target=\"_blank\">stellar-deluge-107</a></strong> to <a href=\"https://wandb.ai/jenninglim/particle-em-mnist\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...............................................................................Epoch 0: 372.041398: Loss 1824.8615521781053\n",
            "...............................................................................Epoch 1: 369.544169: Loss 1215.1942076864123\n",
            "...............................................................................Epoch 2: 369.358910: Loss 1045.6545549223695\n",
            "...............................................................................Epoch 3: 369.395399: Loss 886.3660278320312\n",
            "...............................................................................Epoch 4: 369.397306: Loss 778.2756177684929\n",
            "...............................................................................Epoch 5: 369.296647: Loss 692.9689045193829\n",
            "...............................................................................Epoch 6: 369.483078: Loss 638.0797242756131\n",
            "...............................................................................Epoch 7: 369.405639: Loss 590.8647793154173\n",
            "...............................................................................Epoch 8: 369.366622: Loss 551.5038668475573\n",
            "...............................................................................Epoch 9: 369.373659: Loss 522.8333813631082\n",
            "...............................................................................Epoch 10: 369.208458: Loss 501.81689375865307\n",
            "...............................................................................Epoch 11: 369.387970: Loss 482.6319977965536\n",
            "...............................................................................Epoch 12: 369.326600: Loss 464.72304718403876\n",
            "...............................................................................Epoch 13: 369.233128: Loss 452.25906912888155\n",
            "...............................................................................Epoch 14: 369.230330: Loss 441.64649210096917\n",
            "...............................................................................Epoch 15: 369.241987: Loss 431.90362741977356\n",
            "...............................................................................Epoch 16: 369.268138: Loss 420.89695334132716\n",
            "...............................................................................Epoch 17: 369.401425: Loss 414.1342603466179\n",
            "...............................................................................Epoch 18: 369.237627: Loss 406.8587499690961\n",
            "...............................................................................Epoch 19: 369.354989: Loss 399.7402139011818\n",
            "...............................................................................Epoch 20: 369.277233: Loss 394.9417504419254\n",
            "...............................................................................Epoch 21: 369.126506: Loss 388.4713656268542\n",
            "...............................................................................Epoch 22: 369.363469: Loss 381.33558480950853\n",
            "...............................................................................Epoch 23: 369.356224: Loss 378.1317343410057\n",
            "...............................................................................Epoch 24: 369.243065: Loss 373.2919145415101\n",
            "...............................................................................Epoch 25: 369.364422: Loss 368.3002199583416\n",
            "...............................................................................Epoch 26: 369.168250: Loss 368.1413037263894\n",
            "...............................................................................Epoch 27: 369.355006: Loss 359.30627518665943\n",
            "...............................................................................Epoch 28: 369.157911: Loss 357.4522832556616\n",
            "...............................................................................Epoch 29: 369.205739: Loss 353.8156765322142\n",
            "...............................................................................Epoch 30: 369.213326: Loss 350.7248824880093\n",
            "...............................................................................Epoch 31: 369.344491: Loss 351.01895991458167\n",
            "...............................................................................Epoch 32: 369.250075: Loss 343.27422284476364\n",
            "...............................................................................Epoch 33: 369.258660: Loss 341.5898510896707\n",
            "...............................................................................Epoch 34: 369.211491: Loss 338.12510236909117\n",
            "...............................................................................Epoch 35: 369.382759: Loss 336.6849712902987\n",
            "...............................................................................Epoch 36: 369.221810: Loss 338.6329901972903\n",
            "...............................................................................Epoch 37: 369.302670: Loss 328.69278919847704\n",
            "...............................................................................Epoch 38: 369.077816: Loss 329.12861227687404\n",
            "...............................................................................Epoch 39: 369.347440: Loss 326.81904350956785\n",
            "...............................................................................Epoch 40: 369.299304: Loss 323.4126389901849\n",
            "...............................................................................Epoch 41: 369.194225: Loss 328.48841568186316\n",
            "...............................................................................Epoch 42: 369.287823: Loss 319.2196786614913\n",
            "...............................................................................Epoch 43: 369.310636: Loss 316.03834514376484\n",
            "...............................................................................Epoch 44: 369.353634: Loss 318.0171512652047\n",
            "...............................................................................Epoch 45: 369.462235: Loss 315.6382909847211\n",
            "...............................................................................Epoch 46: 369.123353: Loss 318.94949572599387\n",
            "...............................................................................Epoch 47: 369.347605: Loss 308.32937815219543\n",
            "...............................................................................Epoch 48: 369.472207: Loss 310.45348271237145\n",
            "...............................................................................Epoch 49: 369.696929: Loss 308.2777640427215\n",
            "...............................................................................Epoch 50: 369.236376: Loss 305.5869758702532\n",
            "...............................................................................Epoch 51: 369.423765: Loss 312.6046258467662\n",
            "...............................................................................Epoch 52: 369.151698: Loss 300.3408825065516\n",
            "...............................................................................Epoch 53: 369.381773: Loss 302.95955938025367\n",
            "...............................................................................Epoch 54: 369.386160: Loss 301.0859575875198\n",
            "...............................................................................Epoch 55: 369.256516: Loss 297.85231616829014\n",
            "...............................................................................Epoch 56: 369.457081: Loss 306.84328026107596\n",
            "...............................................................................Epoch 57: 369.046479: Loss 293.6708383680899\n",
            "...............................................................................Epoch 58: 369.341514: Loss 296.46627112279964\n",
            "...............................................................................Epoch 59: 369.134924: Loss 293.96966707253756\n",
            "...............................................................................Epoch 60: 369.113399: Loss 293.8643827800509\n",
            "...............................................................................Epoch 61: 369.204647: Loss 299.4206226204015\n",
            "...............................................................................Epoch 62: 369.131569: Loss 289.0979844105395\n",
            "...............................................................................Epoch 63: 369.320848: Loss 289.3668813584726\n",
            "...............................................................................Epoch 64: 369.119631: Loss 289.9027878000766\n",
            "...............................................................................Epoch 65: 369.144347: Loss 288.9178128785725\n",
            "...............................................................................Epoch 66: 369.419665: Loss 294.92263407646857\n",
            "...............................................................................Epoch 67: 369.165372: Loss 284.30915909779225\n",
            "...............................................................................Epoch 68: 369.146518: Loss 284.00335674044453\n",
            "...............................................................................Epoch 69: 369.180395: Loss 284.29113344602945\n",
            "...............................................................................Epoch 70: 369.077712: Loss 285.9148488105098\n",
            "...............................................................................Epoch 71: 369.105501: Loss 290.8889399661293\n",
            "...............................................................................Epoch 72: 369.274904: Loss 280.5397128334528\n",
            "...............................................................................Epoch 73: 369.240684: Loss 281.18938301183\n",
            "...............................................................................Epoch 74: 369.306866: Loss 280.9526710993127\n",
            "...............................................................................Epoch 75: 369.332297: Loss 280.4931215696697\n",
            "...............................................................................Epoch 76: 369.669453: Loss 287.7004776966723\n",
            "...............................................................................Epoch 77: 369.348362: Loss 277.266635363615\n",
            "...............................................................................Epoch 78: 369.354666: Loss 278.0329669034934\n",
            "...............................................................................Epoch 79: 369.187063: Loss 276.94022707395914\n",
            "...............................................................................Epoch 80: 369.260478: Loss 278.15068768851364\n",
            "...............................................................................Epoch 81: 369.303467: Loss 284.4233203356779\n",
            "...............................................................................Epoch 82: 369.268523: Loss 274.1458929520619\n",
            "...............................................................................Epoch 83: 369.197075: Loss 274.54311148728\n",
            "...............................................................................Epoch 84: 369.232180: Loss 274.5920261431344\n",
            "...............................................................................Epoch 85: 368.946688: Loss 274.5946375207056\n",
            "...............................................................................Epoch 86: 368.911565: Loss 282.5123070825504\n",
            "...............................................................................Epoch 87: 369.174270: Loss 271.63860330702386\n",
            "...............................................................................Epoch 88: 369.145321: Loss 272.0897641725178\n",
            "...............................................................................Epoch 89: 369.182780: Loss 272.17234686356556\n",
            "...............................................................................Epoch 90: 368.956026: Loss 272.1957721951642\n",
            "...............................................................................Epoch 91: 369.214860: Loss 280.37939491754844\n",
            "...............................................................................Epoch 92: 369.200117: Loss 269.42718505859375\n",
            "................................................"
          ]
        }
      ],
      "source": [
        "#@title Main training loop\n",
        "to_range_0_1 = lambda x: (x + 1.) / 2.\n",
        "\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "    project=\"particle-em-mnist\",\n",
        "    config = vars(args),\n",
        ")\n",
        "\n",
        "wandb.watch(model, log=\"all\", log_freq=10)\n",
        "\n",
        "losses = []\n",
        "for epoch in range(args.n_epochs):\n",
        "  model.train()\n",
        "  avg_loss = 0\n",
        "  start = time.time()\n",
        "  for imgs, idx in train:\n",
        "      imgs = imgs.to(device=DEVICE)\n",
        "      loss = pga.step(imgs, idx)\n",
        "      avg_loss += loss\n",
        "      print(\".\", end='')\n",
        "  end = time.time()\n",
        "  avg_loss = avg_loss / len(train) #/ args.n_images\n",
        "  losses.append(avg_loss)\n",
        "\n",
        "\n",
        "  print(f\"Epoch {epoch}: {end - start:2f}: Loss {avg_loss}\")\n",
        "\n",
        "  # Save model\n",
        "  (CHECKPOINT_DIR / wandb.run.name / \"model\").mkdir(exist_ok=True, parents=True)\n",
        "  torch.save(model.state_dict(), CHECKPOINT_DIR / wandb.run.name  / \"model\" / f\"{epoch}_model\")\n",
        "  (CHECKPOINT_DIR / wandb.run.name / \"particles\").mkdir(exist_ok=True, parents=True)\n",
        "  with open(CHECKPOINT_DIR / wandb.run.name / \"particles\" / f\"{epoch}_particles\", 'wb') as f:\n",
        "    pickle.dump(pga._particles, f)\n",
        "  \n",
        "\n",
        "  with torch.no_grad():\n",
        "    n_cols = 8\n",
        "    n_rows = 8\n",
        "    mean = torch.mean(pga._particles, [0, 1, 3, 4])\n",
        "    cov = torch.cov(pga._particles.flatten(0,1).flatten(1, 3).transpose(0, 1))\n",
        "    normal_approx = torch.distributions.multivariate_normal.MultivariateNormal(loc = mean, covariance_matrix=cov)\n",
        "    z = normal_approx.sample(sample_shape=torch.Size([n_cols * n_rows])).unsqueeze(-1).unsqueeze(-1)\n",
        "    samples = to_range_0_1(model(z.to(DEVICE)))\n",
        "    grid = make_grid(samples)\n",
        "    fig = show(grid)\n",
        "    samples = wandb.Image(grid)\n",
        "    (CHECKPOINT_DIR / wandb.run.name / \"grid\").mkdir(exist_ok=True, parents=True)\n",
        "    plt.savefig(CHECKPOINT_DIR / wandb.run.name / \"grid\" / f\"{epoch}_samples.png\", bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    model.eval()\n",
        "    torch.random.manual_seed(1)\n",
        "    original_img = to_range_0_1(train.dataset[0][0].unsqueeze(0))\n",
        "    particle_img = to_range_0_1(model(pga._particles[0, :10].to(DEVICE))).to(original_img.device)\n",
        "    grid = make_grid(torch.concat([original_img, particle_img], dim=0))\n",
        "    particles = wandb.Image(grid)\n",
        "\n",
        "    mse_n_samples = 100\n",
        "    mse_n_particles = args.n_particles\n",
        "    original_img = to_range_0_1(dataset[:mse_n_samples][0].unsqueeze(1))\n",
        "    particle_img = to_range_0_1(model(pga._particles[:mse_n_samples, :mse_n_particles].contiguous().to(DEVICE))).to(original_img.device)\n",
        "    assert original_img.shape == torch.Size([mse_n_samples, 1, 1, 32, 32])\n",
        "    assert particle_img.shape == torch.Size([mse_n_samples, mse_n_particles, 1, 32, 32])\n",
        "    mse = (((particle_img - original_img) ** 2).sum([-1, -2, -3]).mean()).item()\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "    n_missing_img = 10\n",
        "    missing_imgs = dataset[:n_missing_img][0]\n",
        "    init_x = torch.randn(n_missing_img, args.x_dim, 1, 1, requires_grad=True)\n",
        "    opt = torch.optim.Adam([init_x], 1e-2)\n",
        "    mse = torch.nn.MSELoss()\n",
        "    missing_mask = torch.zeros_like(missing_imgs, dtype=torch.bool)\n",
        "\n",
        "    for i in range(10, 22):\n",
        "      for j in range(10, 22):\n",
        "            missing_mask[..., i, j] = True\n",
        "\n",
        "    for i in range(1000):\n",
        "      opt.zero_grad()\n",
        "      filled_imgs = model.forward(init_x.to(DEVICE)).to('cpu')\n",
        "      loss = mse(filled_imgs[~missing_mask], missing_imgs[~missing_mask])\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "\n",
        "    filled_imgs = to_range_0_1(filled_imgs).expand(-1, 3, -1, -1)\n",
        "    missing_imgs = to_range_0_1(missing_imgs).expand(-1, 3, -1, -1)\n",
        "    input = missing_imgs.detach().clone()\n",
        "    input[missing_mask.expand(-1, 3, -1, -1)] = 0.2\n",
        "\n",
        "    for i in range(n_missing_img):\n",
        "      grid = make_grid(torch.concat([input[[i]], filled_imgs[[i]], missing_imgs[[i]]], dim=0))\n",
        "      fig = show(grid)\n",
        "      (CHECKPOINT_DIR / wandb.run.name / \"impaint\" / f\"{epoch}\").mkdir(exist_ok=True, parents=True)\n",
        "      plt.savefig(CHECKPOINT_DIR / wandb.run.name / \"impaint\" / f\"{epoch}\" / f\"{i}.png\", bbox_inches='tight')\n",
        "      plt.close(fig)\n",
        "\n",
        "  if epoch > 2 and args.early_stopping:\n",
        "    if epoch - np.argmin(losses) > 20:\n",
        "      print(\"Early Stop\")\n",
        "      break;\n",
        "\n",
        "    # import matplotlib.pyplot as plt\n",
        "    # particles = pga._particles[:, :mse_n_particles].flatten(0,1).flatten(-3,-1).cpu()\n",
        "    # plt.scatter(particles[:,0], particles[:,1])\n",
        "    # plt.show()\n",
        "  wandb.log({'particles': particles,\n",
        "              'samples': samples,\n",
        "              \"loss\" : avg_loss,\n",
        "              'mse': mse,\n",
        "              'theta_step_size' : pga.theta_opt.param_groups[0]['lr'],\n",
        "              })\n",
        "  plt.close('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E1zwkOW-8f8"
      },
      "outputs": [],
      "source": [
        "plt.close('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_9XA0cSr1lY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MNIST.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}