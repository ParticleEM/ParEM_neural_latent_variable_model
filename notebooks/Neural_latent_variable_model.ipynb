{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNEy6xRqKKxZ"
   },
   "source": [
    "**Warning: Work in progress.** Please keep in that the following is preliminary. Over the following weeks, we will keep refining the current results, adding further results, and cleaning up the code and presentation up until the camera-ready date, so please check back later. Should the paper be accepted we will include this material in a revision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Latent Variable models for image synthesis and in-painting\n",
    "\n",
    "In this example we consider the problem of training neural latent variable models for image synthesis and in-painting tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset description\n",
    "\n",
    "Our datasets are comprised of $M$ images $y = (y^{m})_{m=1}^M$. We consider two datasets:\n",
    "\n",
    "- MNIST containing $70,000$ $d_y:=28\\times 28$ images of hand-written digits: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "- CelebA containing $202,599$ $d_y:=32\\times 32$ images of faces of celebrities: https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
    "\n",
    "In either case we do not use the entire dataset but a randomly subsampled subset of. In what follows, $M$ denotes the size of this training set. Furthermore, all images' pixel values are normalized so that they lie in $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description\n",
    "\n",
    "The model assumes that each image $y^m$ was generated independtly of the others and that it was generated by:\n",
    "\n",
    "1. drawing a latent variable $x^m$ from a zero-mean unit-variance Gaussian distribution $p(x):=\\mathcal{N}(x;0,I_{d_x})$ on a low dimensional latent space ($\\mathbb{R}^{d_x}$ with $d_x$ ranging from $5$ to no more than $100$);\n",
    "2. mapping $x^m$ to the image space via a neural network $f_\\theta$ parametrized by some parameters $\\theta$ in $\\mathbb{R}^{D_\\theta}$;\n",
    "3. adding zero-mean $\\sigma^2$-variance Gaussian noise: $y^m=f_\\theta(x^m)+\\epsilon^m$ where $(\\epsilon^m)_{m=1}^M$ is a an i.i.d. sequence with law $\\mathcal{N}(0,I_{d_y})$.\n",
    "\n",
    "In full, the model's density is given by\n",
    "$$\n",
    "p_\\theta (x,y) = \\prod_{m=1}^M p_\\theta(x^{m}, y^{m})\\qquad\\qquad(1)$$\n",
    "where\n",
    "$$\n",
    "p_\\theta(x^m,y^m)= p_\\theta(y^m|x^m)p(x^m),\\quad\\textrm{with}\\quad p_\\theta(y^m|x^m) := \\mathcal{N}(y^m|f_\\theta(x^m), \\sigma^2 I_{d_y}).\n",
    "$$\n",
    "\n",
    "For $f_\\theta$ we use a convolutional neural network with an architecture emulating that used in \\[[1](https://link.springer.com/chapter/10.1007/978-3-030-58539-6_22)\\], see below for details. In total, it has $3068$ parameters ($D_\\theta=3068$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecure\n",
    "\n",
    "The neural networks has is composed of $4$ basics types of layers:\n",
    "\n",
    "*   $l_\\theta$: fully-connected linear layers,\n",
    "*   $c_\\theta$: convolutional layers,\n",
    "*   $c_\\theta^T$: transpose convolutional layers,\n",
    "*   $b_\\theta:$ batch normalization layers.\n",
    "\n",
    "These are interweaved with GELU activation functions $\\phi$. In particular, they are assembled to $2$ create further types of layers:\n",
    "\n",
    "*   'projection' layers $\\pi_\\theta:=\\phi \\circ b_\\theta\\circ c_\\theta \\circ \\phi\\circ b_\\theta\\circ l_\\theta$;\n",
    "*   'deterministic' layer $d_\\theta=\\phi \\circ b_\\theta \\circ c_\\theta \\circ \\phi\\circ b_\\theta \\circ c_\\theta + I$ where $I$ denotes the identity operator (in other words, the layer has a skip connection).\n",
    "\n",
    "The network itself then consists of a projection layer, followed by two deterministic layers, followed by transpose convolutional layer and a $\\tanh$ activation:\n",
    "\n",
    "$$f_\\theta =  \\tanh\\circ c_\\theta^T\\circ d_\\theta \\circ d_\\theta \\circ \\pi_\\theta$$\n",
    "\n",
    "For more details, please the code in [model.py](). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Training the model entails searching for parameters $\\theta_*$ maximizing the marginal likelihood $\\theta\\mapsto p_\\theta(y):=\\int p_\\theta(x,y)dx$ (or, at least, for a local maximum thereof). To do so, we use PGA slightly modified to better cope with the high evaluation cost of the log-likelihood's, $\\ell(\\theta,x):=\\log(p_\\theta(x,y)$'s, gradients. In particular, in the $\\theta$-update we replace $\\nabla_{\\theta} \\ell(\\theta,x)$ unbiased estimator thereof obtain by subsampling the training set:\n",
    "\n",
    "\\begin{align*}\\nabla_{\\theta} \\ell(\\theta,x)=\\sum{m=1}^M \\nabla_\\theta\\log(p_\\theta(x^m,y^m))=M\\left[\\frac{1}{M}\\sum{m=1}^M \\nabla_\\theta\\log(p_\\theta(x^m,y^m))\\right]\\approx M\\left[\\frac{1}{|\\cal{B}|}\\sum_{m\\in\\mathcal{B}}\\nabla_\\theta\\log(p_\\theta(x^m,y^m))\\right]=\\frac{M}{|\\cal{B}|}\\sum_{m\\in\\mathcal{B}}\\nabla_\\theta\\log(p_\\theta(x^m,y^m)),\\end{align*}\n",
    "\n",
    "where $\\mathcal{B}$ denotes a random subset of $[M]:=\\{1,\\dots, M\\}$ and $|\\mathcal{B}|$ its cardinality. To mitigate the varying magnitudes among $\\nabla_\\theta\\log(p_\\theta(x^m,y^m))$'s entries and improve the learning, we use a modified version of the 'heuristic fix' discussed in Section 2.1 of manuscript: we rescale each entry by scalar only that this time we allow the scalars to vary with the iteration count choosing them like in RMSprop \\[[2]()\\]. In full, we update the parameter estimates $\\theta_k$ using\n",
    "\n",
    "\\begin{align*}\n",
    "    \\theta_{k+1} &= \\theta_k + \\frac{h}{N}\\sum_{n=1}^N \\sum_{m\\in\\mathcal{\\cal{B}_k}} h\\lambda\\Lambda_k\\nabla_\\theta \\log p_{\\theta_k}\n",
    "(X^{n,m}_k, y^{m}) \n",
    "\\end{align*}\n",
    "\n",
    "where $(X^n)_{n=1}^N=((X^{n,m})_{m=1}^M)_{n=1}^N$ denotes the particle cloud at the $k^{th}$ iteration, $\\mathcal{B}_k$ indexes the minibatch used in the $k^{th}$ iteration, $\\Lambda_k$ a diagonal matrix containing the RMSprop step sizes, and $\\lambda$ is a scalar we tune by hand to mitigate differences between the scales of log-likelihood's $\\theta$ and $x$ gradients (it ranges between $0.01$ and $1$).\n",
    "\n",
    "Because the dimensionality of the latent variables is $30$--$600$ times smaller than that of the parameters, the cost of the particle updates is $30$--$600$ smaller than that of the $\\theta$ updates (without subsampling) and, so, we do not have to subsample the $x$ gradients. In particular, we update the particles just as in standard PGA. Given (1), these updates read \n",
    "\n",
    "\\begin{align*}\n",
    "X^{n,m}_{k+1}&=X^{n,m}_k + h\\nabla_x \\log p_{\\theta_k}\n",
    "(X^{n,m}_k, y^{m}) + \\sqrt{2h} W^{n,m}_k \\quad \\forall m\\in\\[M\\],\\enskip n\\in \\[N\\].\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "Describe stopping criterion: early stop bla bla.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image synthesis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-painting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ssu1R-nnaGpW"
   },
   "outputs": [],
   "source": [
    "# Declare dicitonary like object for storing config variables:\n",
    "import argparse\n",
    "args = argparse.Namespace()\n",
    "\n",
    "args.seed = 1  # Seed for PRNs\n",
    "\n",
    "# Data setttings\n",
    "args.n_images = 1000  # M\n",
    "\n",
    "# Training settings\n",
    "args.n_epochs = 500  # K\n",
    "args.n_batch = 128  # M_b\n",
    "args.n_sampler_batch = 750\n",
    "args.early_stopping = True  # Turn on early stopping\n",
    "\n",
    "# Model Settings\n",
    "args.x_dim = 64  # D_x\n",
    "args.theta_opt = 'rmsprop'  # Lambda premultiplying matrix\n",
    "args.likelihood_var = 0.15 ** 2  # \\sigma^2\n",
    "\n",
    "# EM Settings\n",
    "args.theta_step_size = 1e-3  # h_\\theta\n",
    "args.q_step_size = 1e-4  # h_q\n",
    "args.clip_grad = True\n",
    "args.n_particles = 10  # N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceG5fAENPfr6"
   },
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hz47uMXqKYf5"
   },
   "source": [
    "First, we load the modules we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLCqb7XBKj2b"
   },
   "outputs": [],
   "source": [
    "# Install missing modules\n",
    "%%capture\n",
    "!pip install torchtyping\n",
    "!pip install functorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpxVMzKzQGaN"
   },
   "outputs": [],
   "source": [
    "# Import standard modules\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X55L4TEtl385",
    "outputId": "988ce402-e34f-4556-a144-660fef380fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ParEM_VAE'...\n",
      "remote: Enumerating objects: 279, done.\u001b[K\n",
      "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
      "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
      "remote: Total 279 (delta 20), reused 44 (delta 18), pack-reused 230\u001b[K\n",
      "Receiving objects: 100% (279/279), 41.77 KiB | 4.64 MiB/s, done.\n",
      "Resolving deltas: 100% (128/128), done.\n"
     ]
    }
   ],
   "source": [
    "# Import custom modules\n",
    "!rm -rf ParEM_VAE\n",
    "!git clone https://pareem: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@github.com/ParticleEM/ParEM_VAE.git\n",
    "sys.path.append(\"/content/ParEM_VAE/\")\n",
    "from parem.model import G\n",
    "from parem.pga import PGA, optimisers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C83FbhuRRXc_"
   },
   "source": [
    "# Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sqT0ajsqRVKi",
    "outputId": "2ef75704-188e-435d-cb0f-af4dbb33d747"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Mounts drive to VM in colab.\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\", force_remount=False)\n",
    "\n",
    "# Path where dataset will be stored:\n",
    "GDRIVE_DATASET_PATH = Path(\"/content/gdrive/MyDrive/data/vae\")\n",
    "\n",
    "# Path where checkpoints will be saved:\n",
    "CHECKPOINT_DIR = Path(\"/content/gdrive/MyDrive/particle-em/svhn\")\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbrwaBFyRb4P"
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aXOewTu5Piuw",
    "outputId": "85a762f2-793e-4d74-c8c0-e16030d501c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /content/gdrive/MyDrive/data/vae/train_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "#@title Load dataset\n",
    "from parem.svhn import get_svhn\n",
    "\n",
    "dataset = get_svhn(GDRIVE_DATASET_PATH, args.n_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "JtEGiGKkS0pm"
   },
   "outputs": [],
   "source": [
    "#@title Divvy up dataset in batches for training.\n",
    "\n",
    "train = torch.utils.data.DataLoader(dataset, batch_size=args.n_batch, shuffle=True, pin_memory=True)\n",
    "larger_batch_train = torch.utils.data.DataLoader(dataset, batch_size=args.n_sampler_batch, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rduJTaL4RPB7"
   },
   "outputs": [],
   "source": [
    "model = G(args.x_dim, sigma2=args.likelihood_var).to(DEVICE)\n",
    "pga = PGA(model,\n",
    "          args.n_images,\n",
    "          larger_batch_train,\n",
    "          device=DEVICE,\n",
    "          theta_step_size=args.theta_step_size,\n",
    "          q_step_size=args.q_step_size,\n",
    "          n_particles=args.n_particles,\n",
    "          clip_grad=args.clip_grad,\n",
    "          theta_opt=args.theta_opt,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2uiBWF1KPSu"
   },
   "outputs": [],
   "source": [
    "# Import modules necessary for training loop\n",
    "%%capture\n",
    "!pip install wandb\n",
    "import wandb\n",
    "import pickle\n",
    "from torchvision.utils import make_grid\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "weq8_THjRHw-",
    "outputId": "f866f6bf-c48b-4e54-81c1-0dc2bb9cfe79"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjenninglim\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20220806_213722-3e55or9h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jenninglim/particle-em-svhm/runs/3e55or9h\" target=\"_blank\">kind-capybara-62</a></strong> to <a href=\"https://wandb.ai/jenninglim/particle-em-svhm\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........Epoch 0: 4.206381: Loss 46709947.875\n",
      "........Epoch 1: 3.946499: Loss 19369643.375\n",
      "........Epoch 2: 3.942541: Loss 11710681.625\n",
      "........Epoch 3: 3.935411: Loss 9691469.5625\n",
      "........Epoch 4: 3.942672: Loss 7203934.9375\n",
      "........Epoch 5: 3.921455: Loss 5638473.375\n",
      "........Epoch 6: 3.942654: Loss 5491085.0625\n",
      "........Epoch 7: 3.936682: Loss 4494481.84375\n",
      "........Epoch 8: 3.943934: Loss 4375911.15625\n",
      "........Epoch 9: 3.939325: Loss 4216165.15625\n",
      "........Epoch 10: 3.919437: Loss 3672923.21875\n",
      "........Epoch 11: 3.936026: Loss 3611881.6875\n",
      "........Epoch 12: 3.936669: Loss 3467806.5625\n",
      "........Epoch 13: 3.933230: Loss 3397499.40625\n",
      "........Epoch 14: 3.941407: Loss 3179980.125\n",
      "........Epoch 15: 3.920175: Loss 3127742.96875\n",
      "........Epoch 16: 3.935094: Loss 3063833.71875\n",
      "........Epoch 17: 3.937531: Loss 2834152.96875\n",
      "........Epoch 18: 3.932847: Loss 2747621.65625\n",
      "........Epoch 19: 3.982827: Loss 2754412.90625\n",
      "........Epoch 20: 3.914003: Loss 2764356.25\n",
      "........Epoch 21: 3.939615: Loss 2817831.6875\n",
      "........Epoch 22: 3.941590: Loss 2536638.21875\n",
      "........Epoch 23: 3.941233: Loss 2488679.75\n",
      "........Epoch 24: 3.944835: Loss 2443708.4375\n",
      "........Epoch 25: 3.912822: Loss 2464089.015625\n",
      "........Epoch 26: 3.944336: Loss 2316765.484375\n",
      "........Epoch 27: 3.940995: Loss 2376454.4375\n",
      "........Epoch 28: 3.936539: Loss 2308647.71875\n",
      "........Epoch 29: 3.939768: Loss 2215119.34375\n",
      "........Epoch 30: 3.914474: Loss 2153751.84375\n",
      "........Epoch 31: 3.933256: Loss 2142334.328125\n",
      "........Epoch 32: 3.936106: Loss 2086326.65625\n",
      "........Epoch 33: 3.935857: Loss 2215836.125\n",
      "........Epoch 34: 3.935084: Loss 2316128.0625\n",
      "........Epoch 35: 3.945512: Loss 2087454.90625\n",
      "........Epoch 36: 3.938132: Loss 1990762.78125\n",
      "........Epoch 37: 3.942320: Loss 1980859.5625\n",
      "........Epoch 38: 3.979296: Loss 1980256.125\n",
      "........Epoch 39: 3.932307: Loss 1911474.875\n",
      "........Epoch 40: 3.911422: Loss 1907771.5\n",
      "........Epoch 41: 4.036412: Loss 1850947.40625\n",
      "........Epoch 42: 3.938338: Loss 1837492.59375\n",
      "........Epoch 43: 3.930032: Loss 1841067.53125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-48db98b93c53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHECKPOINT_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"{epoch}_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpga\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHECKPOINT_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"{epoch}_opt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"{epoch}_particles\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@title Main training loop\n",
    "to_range_0_1 = lambda x: (x + 1.) / 2.\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"particle-em-svhm\",\n",
    "    config = vars(args),\n",
    ")\n",
    "\n",
    "wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(args.n_epochs):\n",
    "  model.train()\n",
    "  avg_loss = 0\n",
    "  start = time.time()\n",
    "  for imgs, idx in train:\n",
    "      imgs = imgs.to(device=DEVICE)\n",
    "      loss = pga.step(imgs, idx)\n",
    "      avg_loss += loss\n",
    "      print(\".\", end='')\n",
    "  end = time.time()\n",
    "  avg_loss = avg_loss / len(train)\n",
    "\n",
    "  losses.append(avg_loss)\n",
    "  if epoch > 2 and args.early_stopping:\n",
    "    if epoch - np.argmin(losses) > 10:\n",
    "      print(\"Early Stop\")\n",
    "      break;\n",
    "\n",
    "  print(f\"Epoch {epoch}: {end - start:2f}: Loss {avg_loss}\")\n",
    "\n",
    "  # Save model\n",
    "  torch.save(model.state_dict(), CHECKPOINT_DIR / f\"{epoch}_model\")\n",
    "  torch.save(pga.theta_opt.state_dict(), CHECKPOINT_DIR / f\"{epoch}_opt\")\n",
    "  with open(CHECKPOINT_DIR / f\"{epoch}_particles\", 'wb') as f:\n",
    "    pickle.dump(pga._particles, f)\n",
    "  \n",
    "\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    torch.random.manual_seed(1)\n",
    "    sample = to_range_0_1(model(torch.randn(10, args.x_dim,1,1).to(DEVICE)))\n",
    "    grid = make_grid(sample)\n",
    "    samples = wandb.Image(grid)\n",
    "\n",
    "    original_img = to_range_0_1(train.dataset[0][0].unsqueeze(0))\n",
    "    particle_img = to_range_0_1(model(pga._particles[0, :10].to(DEVICE))).to(original_img.device)\n",
    "    grid = make_grid(torch.concat([original_img, particle_img], dim=0))\n",
    "    particles = wandb.Image(grid)\n",
    "\n",
    "    mse_n_samples = 100\n",
    "    mse_n_particles = 10\n",
    "    original_img = to_range_0_1(dataset[:mse_n_samples][0].unsqueeze(1))\n",
    "    particle_img = to_range_0_1(model(pga._particles[:mse_n_samples, :mse_n_particles].to(DEVICE))).to(original_img.device)\n",
    "    assert original_img.shape == torch.Size([mse_n_samples, 1, 3, 32, 32])\n",
    "    assert particle_img.shape == torch.Size([mse_n_samples, mse_n_particles, 3, 32, 32])\n",
    "    mse = (((particle_img - original_img) ** 2).sum([-1, -2, -3]).mean()).item()\n",
    "  wandb.log({'particles': particles,\n",
    "             'samples': samples,\n",
    "             \"loss\" : avg_loss,\n",
    "             'mse': mse,\n",
    "             'theta_step_size' : pga.theta_opt.param_groups[0]['lr'],\n",
    "             })\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural_latent_variable_model.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
